import os
import re
import logging
import traceback
import pandas as pd
from fastapi import FastAPI, Request, Query
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from dotenv import load_dotenv
from mail import enviar_email
from openai import OpenAI
# (demais imports do seu projeto)
from segmento_equivalencias import *

from segmento_equivalencias import buscar_segmentos_em_df_avancado, descricoes_cnae, embeddings_cnae


app = FastAPI()
df_cnae = pd.read_excel("Tabela 14.xlsx")


@app.get("/cnae/segmentos/")
def get_cnae_por_segmentos(segmentos: str = Query(..., description="Segmentos separados por v√≠rgula")):
    termos = [s.strip() for s in segmentos.split(",")]
    df_filtrado = buscar_segmentos_em_df_avancado(
        df_cnae,
        ["Descri√ß√£o"],
        termos,
        embeddings_cnae=embeddings_cnae,
        descricoes_cnae=descricoes_cnae,
        fuzzy_threshold=85,
        emb_threshold=0.7,
        use_embeddings=True
    )
    return df_filtrado.to_dict(orient="records")

# ====== NLTK seguro para Render ======
import nltk
nltk_data_dir = "/tmp/nltk_data"
os.makedirs(nltk_data_dir, exist_ok=True)
nltk.data.path.append(nltk_data_dir)
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', download_dir=nltk_data_dir)
# ================================================================

# ====== IN√çCIO: NOVO BLOCO - LangChain RAG para planilhas =======

from langchain.text_splitter import NLTKTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.documents import Document

def indexar_planilhas_rag():
    arquivos = ['Tabela 14.xlsx', 'canais.xlsx']
    documentos = []
    for arquivo in arquivos:
        if os.path.exists(arquivo):
            df = pd.read_excel(arquivo)
            for idx, row in df.iterrows():
                texto = " | ".join([str(x) for x in row.values])
                documentos.append(
                    Document(page_content=texto, metadata={"source": arquivo, "linha": idx + 1})
                )
            print(f"{arquivo} lido com {df.shape[0]} linhas")
        else:
            print(f"Arquivo n√£o encontrado: {arquivo}")

    if not documentos:
        print("Nenhum documento para indexar!")
        return None

    splitter = NLTKTextSplitter(chunk_size=5, chunk_overlap=1)
    chunks = splitter.split_documents(documentos)
    print(f"Total de chunks: {len(chunks)}")
    embeddings = OpenAIEmbeddings()
    db = FAISS.from_documents(chunks, embeddings)
    print("Indexa√ß√£o RAG conclu√≠da!")
    return db

db_rag = None  # VARI√ÅVEL GLOBAL para reuso se quiser usar buscas posteriormente

# ====== FIM: NOVO BLOCO LangChain RAG ===========================

import logging

STOPWORDS = {
    "para", "com", "sem", "de", "e", "ou", "por", "em", "da", "do",
    "no", "na", "das", "dos"
}

logger = logging.getLogger(__name__)

from rag_engine import (
    filtrar_municipios_por_segmentos_multiplos as filtrar_municipios_por_segmento,
    gerar_tabela_html,
    normalizar_segmentos_inteligente,
    descricoes_cnae,
    embeddings_cnae,
    buscar_cidades_na_openai,
)

from rag_parcerias import buscar_conhecimento

def consultar_taxa_conversao_openai(segmentos):
    """
    Consulta a taxa de convers√£o m√©dia para o(s) segmento(s) informado(s) usando a OpenAI.
    """
    try:
        prompt = (
            f"Para empresas do segmento {', '.join(segmentos)}, qual √© a taxa m√©dia de convers√£o de:\n"
            "- Prospec√ß√£o para oportunidade?\n"
            "- Oportunidade para venda?\n"
            "D√™ um n√∫mero percentual para cada uma, no Brasil, em mercados B2B."
        )
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um consultor especialista em vendas B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            max_tokens=300
        )
        texto = resposta.choices[0].message.content.strip()
        matches = re.findall(r"(\d{1,2})%", texto)
        if len(matches) >= 2:
            p_para_o = int(matches[0]) / 100
            o_para_v = int(matches[1]) / 100
            return p_para_o * o_para_v  # taxa final composta
        else:
            logger.warning("Taxa de convers√£o n√£o encontrada, usando padr√£o 20%")
            return 0.2 if isinstance(segmentos, list) else 0.2
    except Exception as e:
        logger.error(f"Erro ao consultar taxa de conversao: {str(e)}")
        return 0.2 if isinstance(segmentos, list) else 0.2

def novo_diagnostico():
    """Gera um novo dicion√°rio de diagn√≥stico padr√£o."""
    return {
        "nome": None,
        "empresa": None,
        "whatsapp": None,
        "email": None,
        "diagnostico": [],
        "etapa_atual": 0,
        "iniciado": False,
        "municipios": [],
        "resumo": "",
        "mensagem_usuario": "",
        "resposta_ia": ""
    }

# VARI√ÅVEL GLOBAL DO ESTADO DO DIAGN√ìSTICO
data = novo_diagnostico()

# Inicializa√ß√£o do ambiente e OpenAI
load_dotenv()
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Inst√¢ncia do FastAPI e montagem dos arquivos est√°ticos

app.mount("/static", StaticFiles(directory="static"), name="static")

logger.info("Aplicacao FastAPI iniciada")

@app.on_event("startup")
def indexar_automaticamente():
    global db_rag
    try:
        db_rag = indexar_planilhas_rag()  # NOVO: indexa as planilhas no deploy
        from rag_parcerias import indexar_documentos
        total = indexar_documentos()
        logger.info(f"Indexacao automatica no startup concluida com {total} arquivos.")
    except Exception as e:
        logger.warning(f"Indexacao automatica ignorada: {e}")

@app.get("/")
def home():
    """Endpoint padr√£o para servir a p√°gina inicial e resetar o diagn√≥stico."""
    global data
    data = novo_diagnostico()
    logger.info("Pagina inicial acessada e dados resetados")
    return FileResponse("static/index.html")

@app.get("/chat")
def chat_get():
    """Bloqueia acesso GET na rota de chat."""
    return {"erro": "Metodo GET nao permitido nesta rota. Use POST com corpo JSON contendo 'mensagem'."}

class Mensagem(BaseModel):
    mensagem: str

perguntas = [
    "Qual o site da empresa?",
    "Quais segmentos a empresa atende?",
    "Poderia citar 3 clientes atuais?",
    "Quais dores a sua empresa resolve e quais beneficios sao levados aos clientes?",
    "Quais os seus diferenciais?",
    "Quais os produtos e servicos vendidos?",
    "Qual o modelo de negocio da empresa? Comercializa licencas? Cobra mensalidade? Cobra projeto?",
    "Qual o ticket medio dos negocios?",
    "Qual o ciclo medio de vendas (em dias)?",
    "Qual a sua expectativa de vendas de novos clientes pelos canais mensalmente?"
]

@app.post("/chat")
async def chat(req: Request):
    """
    Endpoint principal do chat. Recebe perguntas e coleta as informa√ß√µes para o diagn√≥stico.
    """
    body = await req.json()
    msg = body.get("mensagem", "").strip()
    logger.info(f"Mensagem recebida: {msg}")

    if not data["iniciado"]:
        data["iniciado"] = True
        return {"mensagem": "Ola... Para comecarmos o diagnostico, me fale de onde voc√™ fala..."}

    if not data.get("origem"):
        data["origem"] = msg
        return {"pergunta": "Qual o seu nome?"}

    if not data["nome"]:
        data["nome"] = msg
        return {"pergunta": "Qual o nome da sua empresa?"}

    if not data["empresa"]:
        data["empresa"] = msg
        return {"pergunta": "Qual o seu WhatsApp? (Ex: DDD9XXXXYYYY)"}

    if not data["whatsapp"]:
        msg = re.sub(r"\D", "", msg)
        if re.match(r"^\d{11}$", msg):
            data["whatsapp"] = msg
            return {"pergunta": "Qual o seu e-mail?"}
        logger.warning("WhatsApp em formato invalido")
        return {"pergunta": "Formato invalido. Exemplo: DDD9XXXXYYYY"}

    if not data["email"]:
        if re.match(r"^[\w\.-]+@[\w\.-]+\.\w{2,4}$", msg):
            data["email"] = msg
            return {"mensagem": "Obrigado! Agora vamos entender melhor sua empresa.", "pergunta": perguntas[0]}
        logger.warning("E-mail invalido")
        return {"pergunta": "E-mail invalido. Envie um formato valido."}

    if data["etapa_atual"] < len(perguntas):
        if data["etapa_atual"] in [7, 8, 9]:
            valor = re.sub(r"[^\d]", "", msg)
            if not valor.isdigit():
                return {"pergunta": "Por favor, responda apenas com numeros (sem R$, pontos ou virgulas)."}
            msg = valor

        data["diagnostico"].append(msg)
        logger.info(f"Respostas coletadas at√© agora: {len(data['diagnostico'])}")

        data["etapa_atual"] += 1
        if data["etapa_atual"] < len(perguntas):
            return {"pergunta": perguntas[data["etapa_atual"]]}
        else:
            if len(data["diagnostico"]) < len(perguntas):
                logger.error("Diagn√≥stico incompleto. Esperado respostas.")
                return {
                    "mensagem": "Erro: Respostas incompletas. Por favor, reinicie o diagn√≥stico.",
                    "resumo": "Erro: respostas insuficientes.",
                    "email": data["email"]
                }
            try:
                prompt = gerar_prompt(data)
                data["prompt"] = prompt
                data["finalizado"] = True
                logger.info("Prompt preparado com sucesso")
                return {
                    "mensagem": "Analisando suas respostas e preparando o diagn√≥stico...",
                    "loading": True,
                    "email": data["email"]
                }
            except Exception as e:
                print(f"Erro ao preparar prompt: {e}")
                return {
                    "resumo": f"Erro t√©cnico: {str(e)}"
                }

    return {"mensagem": "Diagnostico ja concluido."}

@app.post("/gerar-diagnostico")
async def gerar_diagnostico():
    """
    Endpoint para gerar o diagn√≥stico final e enviar por e-mail.
    """
    try:
        # Agora, gerar_prompt retorna dois valores:
        prompt_sem_cidades_html, cidades_html = gerar_prompt(data)
        texto_diagnostico = chamar_llm(prompt_sem_cidades_html)
        diagnostico_final = (
            texto_diagnostico +
            "\n\nüîπ Cidades com Potencial (N√ÉO ALTERAR O BLOCO ABAIXO - HTML TABELA):\n" +
            cidades_html
        )

        diagnostico_final_limpo = limpar_tabela_fake(diagnostico_final)
        diagnostico_formatado = titulos_html_sem_hash(diagnostico_final_limpo)


        enviar_email(data, diagnostico_formatado, copia_para=["alexandre.maia@acp.tec.br"])
        logger.info("Diagnostico gerado com sucesso pela LLM e e-mail enviado")
        return {
            "mensagem": "Diagnostico finalizado! Aqui esta nossa analise baseada nas suas respostas:",
            "resumo": diagnostico_formatado,
            "email": data["email"]
        }
    except Exception as e:
        logger.error(f"Erro ao gerar diagnostico: {str(e)}")
        return {
            "mensagem": "Ocorreu um erro ao gerar o diagnostico.",
            "resumo": f"Erro ao gerar sugestao: {str(e)}\n\n{traceback.format_exc()}",
            "email": data["email"]
        }

@app.post("/reset")
async def resetar_diagnostico():
    """Reseta o estado do diagn√≥stico global."""
    global data
    data = novo_diagnostico()
    return {"status": "resetado"}

@app.post("/reindexar-rag")
async def reindexar_rag():
    global db_rag
    try:
        db_rag = indexar_planilhas_rag()  # NOVO: reindexa as planilhas manualmente
        from rag_parcerias import indexar_documentos
        total = indexar_documentos()
        logger.info(f"Reindexacao realizada com {total} arquivos")
        return {"status": "ok", "arquivos_indexados": total}
    except Exception as e:
        logger.error(f"Erro na reindexacao do RAG: {str(e)}")
        return {
            "status": "erro",
            "mensagem": str(e),
            "detalhes": traceback.format_exc()
        }
    
def truncar_texto(texto, limite=500):
    """Trunca um texto se for maior do que o limite definido."""
    return texto[:limite] + "..." if len(texto) > limite else texto

def gerar_prompt(data):
    """
    Monta o prompt estruturado para an√°lise do diagn√≥stico e gera√ß√£o do relat√≥rio.
    Inclui o bloco de cidades em HTML no local correto do texto final.
    """
    origem = data.get("origem", "")
    nome = data["nome"]
    empresa = data["empresa"]
    respostas = data["diagnostico"]




    if len(respostas) < 2:
        raise ValueError("Respostas insuficientes para gerar o prompt.")

    site = respostas[0]
    segmentos_raw = respostas[1]
    clientes = respostas[2]
    dores = respostas[3]
    diferenciais = respostas[4]
    produtos = respostas[5]
    modelo_negocio = respostas[6]
    ticket_medio_str = respostas[7]
    ciclo_vendas = respostas[8]
    meta_clientes = respostas[9]

    if isinstance(segmentos_raw, list):
        segmentos_raw = " ".join(segmentos_raw)

    segmento_original = segmentos_raw if len(respostas) > 1 else ""

    # Lista de segmentos limpa
    segmentos_lista = [
        seg.strip().lower()
        for seg in re.split(r"[,\s]+", segmento_original)
        if seg.strip().lower() not in STOPWORDS and len(seg.strip()) > 2
    ]

    segmentos_normalizados = []
    for termo in segmentos_lista:
        normalizados = normalizar_segmentos_inteligente(termo, descricoes_cnae, embeddings_cnae)
        segmentos_normalizados.extend(normalizados)

    segmento = truncar_texto(segmentos_raw)
    clientes = truncar_texto(clientes)
    dores = truncar_texto(dores)
    produtos = truncar_texto(produtos)
    modelo = truncar_texto(modelo_negocio)

    resumo_empresa = detalhar_empresa_openai(site, empresa)
    resumo_produto = detalhar_produto_openai(produtos, segmento)
    resumo_clientes = detalhar_clientes_openai(clientes)

    taxa_pros, taxa_vendas = consultar_taxas_software_b2b()

    try:
        novos_clientes = int(meta_clientes)
        ciclo = int(ciclo_vendas)
    except Exception:
        novos_clientes = 1
        ciclo = 90

    oportunidades = int(novos_clientes / taxa_vendas)
    prospeccoes = int(oportunidades / taxa_pros)

    conhecimento_modelos = buscar_conhecimento_complementado(
        f"Quais modelos de parceria s√£o ideais para empresas como a {empresa}, que atuam com {produtos} no segmento {segmento}?"
    )
    conhecimento_perfis = buscar_conhecimento_complementado(
        f"Quais os perfis de empresas parceiras ideais para {empresa}, considerando que seus clientes s√£o como: {clientes}?"
    )
    conhecimento_servicos = buscar_conhecimento_complementado(
        f"Que servi√ßos agregados s√£o relevantes para empresas que vendem {produtos}, com modelo de neg√≥cio baseado em {modelo}?"
    )

    # Busca as cidades (RAG + OpenAI para completar 30)
    
    if isinstance(segmento_original, list):
        segmento_original = " ".join(segmento_original)
    cidades_df = filtrar_municipios_por_segmento(segmento_original, top_n=30)


    print("==== DEBUG cidades_df ====")
    print(cidades_df.head(10))
    print("==== FIM DEBUG cidades_df ====")


    for col in ["Empresas_Segmento", "Empresas_Perfil_Canal"]:
        if col in cidades_df.columns:
            cidades_df[col] = pd.to_numeric(cidades_df[col], errors="coerce").fillna(0).astype(int)

    if cidades_df.empty or any(col not in cidades_df.columns for col in ["Municipio", "Empresas_Segmento", "Empresas_Perfil_Canal"]):
        logger.warning("DataFrame de cidades incompleto. Criando estrutura padr√£o.")
        cidades_df = pd.DataFrame({
            "Municipio": ["CidadeDesconhecida"],
            "Empresas_Segmento": [0],
            "Empresas_Perfil_Canal": [0]
        })

    for col in ["Municipio", "Empresas_Segmento", "Empresas_Perfil_Canal"]:
        if col not in cidades_df.columns:
            cidades_df[col] = 0 if col != "Municipio" else "CidadeDesconhecida"

    try:
        cidades_df = cidades_df.sort_values(by="Empresas_Segmento", ascending=False).reset_index(drop=True)
    except Exception as e:
        logger.error(f"[FALHA] Erro ao ordenar DataFrame por 'Empresas_Segmento': {e}")

    if len(cidades_df) < 30:
        if cidades_df.empty or "Municipio" not in cidades_df.columns:
            cidades_df = pd.DataFrame({
                "Municipio": ["CidadeDesconhecida"],
                "Empresas_Segmento": [0],
                "Empresas_Perfil_Canal": [0]
            })
        cidades_existentes = cidades_df["Municipio"].tolist()
        faltantes = 30 - len(cidades_df)
        cidades_df_extra = buscar_cidades_na_openai(segmentos_lista, cidades_existentes, faltantes)
        for col in ["Municipio", "Empresas_Segmento", "Empresas_Perfil_Canal"]:
            if col not in cidades_df_extra.columns:
                cidades_df_extra[col] = 0 if col != "Municipio" else "CidadeDesconhecida"
        cidades_df = pd.concat([cidades_df, cidades_df_extra], ignore_index=True)
        cidades_df = cidades_df.drop_duplicates(subset="Municipio").reset_index(drop=True)
        faltam = 30 - len(cidades_df)
        if faltam > 0:
            cidades_df_extra = buscar_cidades_na_openai(
                segmentos_lista, cidades_df["Municipio"].tolist(), faltam
            )
            cidades_df = pd.concat(
                [cidades_df, cidades_df_extra], ignore_index=True
            ).drop_duplicates(subset="Municipio").reset_index(drop=True)
        for col in ["Municipio", "Empresas_Segmento", "Empresas_Perfil_Canal"]:
            if col not in cidades_df.columns:
                cidades_df[col] = 0 if col != "Municipio" else "CidadeDesconhecida"
        cidades_df = cidades_df.sort_values(by="Empresas_Segmento", ascending=False).reset_index(drop=True)

    cidades_html = gerar_tabela_html(cidades_df)

    print("==== DEBUG cidades_html ====")
    print(cidades_html)
    print("==== FIM DEBUG cidades_html ====")

    try:
        ticket = float(ticket_medio_str)
        ciclo = int(ciclo_vendas)
        novos_clientes = int(meta_clientes)
    except Exception as e:
        raise ValueError(f"Erro ao converter valores numericos: {str(e)}") from e

    conversao = consultar_taxa_conversao_openai(segmentos_normalizados)
    try:
        conversao = float(conversao)
        if conversao <= 0 or conversao > 1:
            raise ValueError("Convers√£o fora do intervalo aceit√°vel.")
    except Exception as e:
        logger.warning(f"Convers√£o inv√°lida detectada ({conversao}): {str(e)}. Usando valor padr√£o 0.2")
        conversao = 0.2

    if not isinstance(conversao, (float, int)) or conversao == 0:
        logger.warning(f"Convers√£o inv√°lida detectada: {conversao}. Usando valor padr√£o 0.2")
        conversao = 0.2

    oportunidades = int(novos_clientes / conversao)
    prospeccoes = int(oportunidades / conversao)

    from datetime import datetime, timedelta
    receita_24_meses = 0
    hoje = datetime.today()
    for canal_index in range(20):
        dias_ate_primeira_venda = 90 + ciclo + (canal_index * 30)
        data_primeira_venda = hoje + timedelta(days=dias_ate_primeira_venda)
        meses_restantes = max(0, 24 - ((data_primeira_venda - hoje).days // 30))
        receita_24_meses += ticket * meses_restantes

    receita_mensal = ticket * novos_clientes * 20

    # Retorna diagn√≥stico estruturado com bloco das cidades em HTML inclu√≠do


    prompt_sem_cidades_html = f'''
üß† DIAGN√ìSTICO ESTRUTURADO ‚Äì EMPRESA {empresa.upper()}

üîπ Local informado:
{origem}

üîπ Resumo sobre a empresa:
{resumo_empresa}

üîπ Produtos e Servi√ßos:
{resumo_produto}

üîπ Perfil dos Clientes atendidos:
{resumo_clientes}

üîπ Dores e Benef√≠cios:
{dores}

üîπ Modelo de Neg√≥cio:
{modelo_negocio}

üîπ Necessidades de Prospec√ß√£o:
- Meta Prevista: {novos_clientes} novo cliente/m√™s  
- Ciclo de Venda: {ciclo} dias  
- √çndice de Convers√£o de Prospec√ß√£o em Oportunidade: {int(taxa_pros * 100)}%  
- N√∫mero de Prospec√ß√µes: {prospeccoes:,}  
- √çndice de Convers√£o de Oportunidades em Vendas: {int(taxa_vendas * 100)}%  
- N√∫mero de Oportunidades: {oportunidades:,}

üîπ Modelos de Parceria:
{conhecimento_modelos}

üîπ Perfis de Empresas Parceiras com FIT:
{conhecimento_perfis}

üîπ Servi√ßos Agregados:
{conhecimento_servicos}

üîπ Retorno sobre o Investimento:
Ticket M√©dio: R${ticket:,.2f}
Receita Mensal com 20 canais: R${receita_mensal:,.2f}
Receita estimada em 24 meses: R${receita_24_meses:,.2f}

üîπ Dicas Estrat√©gicas:
- Inicie com 3 a 5 canais
- Forne√ßa treinamentos e acompanhamento semanal
- Crie campanhas e a√ß√µes de marketing
- Corrija falhas com feedback dos canais

üîπ Chamada para A√ß√£o:
Sua empresa est√° pronta para crescer com uma estrat√©gia s√≥lida de canais de vendas.
Entre em contato com a AC Partners e comece agora o onboarding comercial com especialistas.
'''

    # RETORNE O PROMPT E O HTML DAS CIDADES SEPARADOS!
    return prompt_sem_cidades_html, cidades_html

def chamar_llm(prompt):
    """Chama a OpenAI para complementar o diagn√≥stico."""
    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Voc√™ √© um consultor especialista em canais de vendas. "
                        "Siga exatamente a estrutura em 13 partes numeradas conforme o prompt. "
                        "Quando houver tabela HTML (como o bloco de Cidades com Potencial), mantenha o conte√∫do original sem reescrever, resumir ou excluir. "
                        "Seu papel √© complementar, n√£o modificar a estrutura do prompt."
                    )
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=2048
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error("Erro na chamada a API OpenAI")
        raise RuntimeError("Erro na chamada a API OpenAI.") from e

# Fun√ß√µes auxiliares de detalhamento e conhecimento (sem altera√ß√£o, j√° estavam bem formatadas)
def sugerir_cidades_openai(segmentos, total_necessario=30):
    try:
        prompt = (
            f"Liste {total_necessario} cidades brasileiras com potencial de mercado para empresas que atuam nos segmentos: {', '.join(segmentos)}. "
            "Considere demanda, perfil econ√¥mico, presen√ßa de agroneg√≥cio, ind√∫stria ou com√©rcio local, dependendo do segmento. "
            "Forne√ßa apenas os nomes das cidades, uma por linha."
        )
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um analista de intelig√™ncia de mercado."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
            max_tokens=2000
        )
        cidades = resposta.choices[0].message.content.strip().split("\n")
        return [c.strip().split("-")[0].strip() for c in cidades if c.strip()]
    except Exception as e:
        logger.error(f"Erro ao sugerir cidades com OpenAI: {str(e)}")
        return []

def detalhar_empresa_openai(site, nome_empresa):
    try:
        prompt = (
            f"Voc√™ √© um redator t√©cnico. Com base no nome da empresa \"{nome_empresa}\" e no site \"{site}\", "
            "gere um resumo claro, profissional e objetivo sobre a empresa. "
            "Destaque o segmento de atua√ß√£o, diferenciais e √°reas atendidas. Use no m√°ximo 3 frases."
        )
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um redator corporativo especializado em empresas B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        return resposta.choices[0].message.content.strip()
    except Exception as e:
        logger.warning(f"Erro ao detalhar empresa: {str(e)}")
        return f"Para mais informa√ß√µes, visite [{nome_empresa}]({site})."

def detalhar_produto_openai(produto_texto, segmento):
    try:
        prompt = (
            f"Explique de forma clara e profissional o seguinte produto ou solu√ß√£o voltado ao segmento {segmento}: \"{produto_texto}\". "
            "Gere 2 frases destacando utilidade e valor estrat√©gico para empresas B2B."
        )
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um especialista em posicionamento de solu√ß√µes B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        return resposta.choices[0].message.content.strip()
    except Exception as e:
        logger.warning(f"Erro ao detalhar produto: {str(e)}")
        return produto_texto

def detalhar_clientes_openai(clientes_lista):
    try:
        prompt = (
            "Gere um par√°grafo curto sobre o perfil dos seguintes clientes atendidos atualmente:\n"
            f"{clientes_lista}\nMostre diversidade, relev√¢ncia setorial e valor estrat√©gico."
        )
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um analista de portf√≥lio de clientes B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4
        )
        return resposta.choices[0].message.content.strip()
    except Exception as e:
        logger.warning(f"Erro ao detalhar clientes: {str(e)}")
        return clientes_lista

def consultar_taxas_software_b2b():
    prompt = (
        "No mercado brasileiro de software B2B:\n\n"
        "1. Qual √© a taxa m√©dia de convers√£o de prospec√ß√µes em oportunidades comerciais?\n"
        "2. Qual √© a taxa m√©dia de convers√£o de oportunidades em vendas fechadas?\n\n"
        "Forne√ßa os dois valores em porcentagem. Exemplo:\n"
        "- Prospec√ß√£o para Oportunidade: 10%\n"
        "- Oportunidade para Venda: 20%\n"
    )
    try:
        resposta = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um especialista em dados de mercado e vendas B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3
        )
        texto = resposta.choices[0].message.content.strip()
        logger.info(f"Resposta OpenAI sobre convers√£o: {texto}")

        matches = re.findall(r"(\d{1,3})%", texto)
        if len(matches) >= 2:
            taxa_prospeccao = int(matches[0]) / 100
            taxa_vendas = int(matches[1]) / 100
            return taxa_prospeccao, taxa_vendas
        else:
            logger.warning("N√£o foi poss√≠vel extrair as duas taxas. Usando padr√£o.")
            return 0.1, 0.2
    except Exception as e:
        logger.error(f"Erro ao consultar taxas de convers√£o: {str(e)}")
        return 0.1, 0.2

def buscar_conhecimento_complementado(pergunta: str) -> str:
    """
    Consulta o RAG e complementa com a OpenAI para respostas mais completas.
    Garante resposta robusta para perguntas-chave do diagn√≥stico.
    """
    try:
        resposta_rag = buscar_conhecimento(pergunta, k=3)
        prompt = (
            "Voc√™ √© um consultor de canais de vendas B2B. Responda √† pergunta abaixo de forma clara, objetiva "
            "e baseada em boas pr√°ticas e exemplos do mercado brasileiro.\n\n"
            f"Pergunta: {pergunta}\n\n"
            f"A resposta deve ser complementar a este conte√∫do j√° obtido:\n{resposta_rag}\n"
        )
        resposta_openai = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "Voc√™ √© um consultor de canais de vendas com experi√™ncia em parcerias B2B."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.4,
            max_tokens=600
        )
        return f"{resposta_rag}\n\nüîé Complemento da IA:\n{resposta_openai.choices[0].message.content.strip()}"
    except Exception as e:
        logger.warning(f"Erro ao complementar conhecimento com OpenAI: {str(e)}")
        return buscar_conhecimento(pergunta)
    
def limpar_tabela_fake(diagnostico_final):
    # Procure o marcador do bloco real:
    marcador = "üîπ Cidades com Potencial (N√ÉO ALTERAR O BLOCO ABAIXO - HTML TABELA):"
    partes = diagnostico_final.split(marcador)
    if len(partes) == 2:
        # S√≥ retorna o texto at√© o marcador + o marcador + a tabela real depois
        texto_antes = partes[0].strip()
        bloco_real = marcador + partes[1]
        return f"{texto_antes}\n\n{bloco_real}"
    return diagnostico_final  # fallback se n√£o encontrar

def titulos_html_sem_hash(texto):
    padrao = r"^###\s*([\d]+\. [^\n]+)"
    texto = re.sub(padrao, r"<strong>\1</strong>\n\n", texto, flags=re.MULTILINE)
    return texto